{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed87b53b-9602-4c27-ba3f-d9d40ff4279a",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Atari Games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a621639-4192-4fce-895e-14b335bfe7ff",
   "metadata": {},
   "source": [
    "## 1. Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9380453d-9c2b-41fb-b9f3-b7d739143812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.create import create_model, get_utility_params\n",
    "from utils.helper import set_device\n",
    "from utils.model_utils import load_model\n",
    "from utils.render import video_render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4292c565-0b2d-45ed-98bf-4b43d7cb5824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_EPISODES=100000, SAVE_EVERY=10000\n"
     ]
    }
   ],
   "source": [
    "# Get utility parameters from yaml file\n",
    "util_params = get_utility_params()\n",
    "\n",
    "# Set them as hyperparameters\n",
    "NUM_EPISODES = util_params['num_episodes']\n",
    "SAVE_EVERY = util_params['save_every']\n",
    "print(f'NUM_EPISODES={NUM_EPISODES}, SAVE_EVERY={SAVE_EVERY}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1030fb-367b-4956-a090-2ecfc8a80eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA unavailable. Device set to CPU -> 'cpu'.\n"
     ]
    }
   ],
   "source": [
    "# Set CUDA device\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb40765c-056c-4a44-9638-18528853cb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env2=ALE/Qbert-v5, env3=ALE/MontezumaRevenge-v5\n"
     ]
    }
   ],
   "source": [
    "env2 = util_params['env_2']\n",
    "env3 = util_params['env_3']\n",
    "print(f'env2={env2}, env3={env3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da63c80-1fd1-4579-b18a-d5fa6c20f1a0",
   "metadata": {},
   "source": [
    "## 2. Model Creation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371fd8e-4667-4bd0-93f9-99770ce70873",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2a. Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4d531e1-2dd7-4ffa-9e9a-3ae773844c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create DQN instance\n",
    "# dqn = create_model('dqn', device=device, im_type='curiosity')\n",
    "# dqn2 = create_model('dqn', device=device)\n",
    "# dqn3 = create_model('dqn', device=device, im_type='empowerment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c75b7a4-5c68-4316-820a-b459ee30dafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'gym_name': 'ALE/SpaceInvaders-v5', 'name': 'SpaceInvaders', 'obs_space': Box(0, 255, (4, 84, 84), uint8), 'action_space': Discrete(6), 'input_shape': (4, 84, 84), 'n_actions': 6, 'img_size': 84, 'stack_size': 4, 'capture_video': False, 'record_every': 10000},\n",
       " {'gym_name': 'ALE/SpaceInvaders-v5', 'name': 'SpaceInvaders', 'obs_space': Box(0, 255, (4, 84, 84), uint8), 'action_space': Discrete(6), 'input_shape': (4, 84, 84), 'n_actions': 6, 'img_size': 84, 'stack_size': 4, 'capture_video': False, 'record_every': 10000},\n",
       " {'gym_name': 'ALE/SpaceInvaders-v5', 'name': 'SpaceInvaders', 'obs_space': Box(0, 255, (4, 84, 84), uint8), 'action_space': Discrete(6), 'input_shape': (4, 84, 84), 'n_actions': 6, 'img_size': 84, 'stack_size': 4, 'capture_video': False, 'record_every': 10000})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dqn.env_details, dqn2.env_details, dqn3.env_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56824d63-5850-400d-ac3a-4684b378bbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on SpaceInvaders with 5 episodes.\n",
      "Buffer size: 1k, batch size: 32, max timesteps: 1k, num network updates: 4, intrinsic method: curiosity\n",
      "(1.0/5) Episode Score: 120,  Train Loss: 28244897020206741716992.00000,  Curiosity Loss: 1844.45471,  Time taken: 19 secs.\n",
      "(2.0/5) Episode Score: 65,  Train Loss: 297685826675757859676880896.00000,  Curiosity Loss: 1882.88391,  Time taken: 18 secs.\n",
      "(3.0/5) Episode Score: 80,  Train Loss: 604445200933003826183536640.00000,  Curiosity Loss: 1815.59290,  Time taken: 17 secs.\n",
      "(4.0/5) Episode Score: 155,  Train Loss: 1042372528556347067987918848.00000,  Curiosity Loss: 1849.64197,  Time taken: 26 secs.\n",
      "(5.0/5) Episode Score: 155,  Train Loss: 1080812517939929685189197824.00000,  Curiosity Loss: 1780.05713,  Time taken: 20 secs.\n",
      "Saved model at episode 5 as: 'dqncuriosity_batch32_SpaInv_ep5.pt'.\n",
      "Saved logger data to 'saved_models/dqncuriosity_SpaInv_logger_data.tar.gz'. Total size: 852 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "# # Train model\n",
    "# dqn.train(num_episodes=5, print_every=1, save_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d23ebf8-43b6-46e8-a14f-68c01e391598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on SpaceInvaders with 5 episodes.\n",
      "Buffer size: 1k, batch size: 32, max timesteps: 1k, num network updates: 4, intrinsic method: None\n",
      "(1.0/5) Episode Score: 215,  Train Loss: 8929011000602317155205120.00000,  Time taken: 10 secs.\n",
      "(2.0/5) Episode Score: 290,  Train Loss: 668769292665934212021354496.00000,  Time taken: 13 secs.\n",
      "(3.0/5) Episode Score: 80,  Train Loss: 967477935960347045206687744.00000,  Time taken: 4 secs.\n",
      "(4.0/5) Episode Score: 105,  Train Loss: 1458471454982388323174055936.00000,  Time taken: 7 secs.\n",
      "(5.0/5) Episode Score: 260,  Train Loss: 1700385746113830124976603136.00000,  Time taken: 11 secs.\n",
      "Saved model at episode 5 as: 'dqn_batch32_SpaInv_ep5.pt'.\n",
      "Saved logger data to 'saved_models/dqn_SpaInv_logger_data.tar.gz'. Total size: 808 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "# dqn2.train(num_episodes=5, print_every=1, save_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59581597-64b7-4813-aed5-7e9cda820114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on SpaceInvaders with 5 episodes.\n",
      "Buffer size: 1k, batch size: 32, max timesteps: 1k, num network updates: 4, intrinsic method: empowerment\n",
      "(1.0/5) Episode Score: 15,  Train Loss: 0.00935,  Source Loss: 0.03580,  Forward Loss: 1.16270,  Time taken: 5 secs.\n",
      "(2.0/5) Episode Score: 110,  Train Loss: 7.36104,  Source Loss: 0.03171,  Forward Loss: 1.25821,  Time taken: 6 secs.\n",
      "(3.0/5) Episode Score: 240,  Train Loss: 2514.66577,  Source Loss: 0.03427,  Forward Loss: 0.53753,  Time taken: 10 secs.\n",
      "(4.0/5) Episode Score: 135,  Train Loss: 30789.06836,  Source Loss: 0.03172,  Forward Loss: 0.19171,  Time taken: 6 secs.\n",
      "(5.0/5) Episode Score: 135,  Train Loss: 412146.56250,  Source Loss: 0.03403,  Forward Loss: 0.09606,  Time taken: 8 secs.\n",
      "Saved model at episode 5 as: 'dqnempowerment_batch32_SpaInv_ep5.pt'.\n",
      "Saved logger data to 'saved_models/dqnempowerment_SpaInv_logger_data.tar.gz'. Total size: 905 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "# dqn3.train(num_episodes=5, print_every=1, save_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea56055b-a0d3-4b58-9c5e-c74c3d1083d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Available attributes: '['actions', 'train_losses', 'ep_scores', 'intrinsic_losses']',\n",
       " Available attributes: '['actions', 'train_losses', 'ep_scores', 'intrinsic_losses']',\n",
       " Available attributes: '['actions', 'train_losses', 'ep_scores', 'intrinsic_losses']')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dqn.logger, dqn2.logger, dqn3.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e59db80a-3efa-4e8f-9053-24691b685117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Counter({4: 389, 1: 399, 0: 397, 2: 373, 5: 379, 3: 430})],\n",
       " [Counter({0: 536, 2: 459, 3: 540, 1: 479, 5: 493, 4: 505})],\n",
       " [Counter({2: 406, 3: 428, 0: 413, 5: 426, 1: 456, 4: 423})])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dqn.logger.actions, dqn2.logger.actions, dqn3.logger.actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60affa60-f650-4b74-89de-d59f8a6b7b04",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2b. Rainbow Deep Q-Network (RDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "725535d6-9d22-4398-85fc-0ffb43650a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Rainbow DQN instance\n",
    "rainbow = create_model('rainbow', env=env2, device=device, im_type='curiosity')\n",
    "rainbow2 = create_model('rainbow', env=env2, device=device)\n",
    "rainbow3 = create_model('rainbow', env=env2, device=device, im_type='empowerment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6aad46cd-adc2-4828-b283-2cd7fc2fbc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'gym_name': 'ALE/Qbert-v5', 'name': 'Qbert', 'obs_space': Box(0, 255, (4, 84, 84), uint8), 'action_space': Discrete(6), 'input_shape': (4, 84, 84), 'n_actions': 6, 'img_size': 84, 'stack_size': 4, 'capture_video': False, 'record_every': 10000},\n",
       " {'gym_name': 'ALE/Qbert-v5', 'name': 'Qbert', 'obs_space': Box(0, 255, (4, 84, 84), uint8), 'action_space': Discrete(6), 'input_shape': (4, 84, 84), 'n_actions': 6, 'img_size': 84, 'stack_size': 4, 'capture_video': False, 'record_every': 10000},\n",
       " {'gym_name': 'ALE/Qbert-v5', 'name': 'Qbert', 'obs_space': Box(0, 255, (4, 84, 84), uint8), 'action_space': Discrete(6), 'input_shape': (4, 84, 84), 'n_actions': 6, 'img_size': 84, 'stack_size': 4, 'capture_video': False, 'record_every': 10000})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rainbow.env_details, rainbow2.env_details, rainbow3.env_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f681e4b3-54d7-4fc8-91a3-69a7162f1f54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on Qbert with 5 episodes.\n",
      "Buffer size: 1k, batch size: 32, max timesteps: 1k, num network updates: 4, replay period: 100, intrinsic method: curiosity.\n",
      "(1.0/5)  Episode Score: 0,  Train Loss: 7.34908,  Curiosity Loss: 3.54450,  Time taken: 13 secs.\n",
      "(2.0/5)  Episode Score: 200,  Train Loss: 7.33582,  Curiosity Loss: 3.54445,  Time taken: 13 secs.\n",
      "(3.0/5)  Episode Score: 0,  Train Loss: 7.36223,  Curiosity Loss: 3.54735,  Time taken: 12 secs.\n",
      "(4.0/5)  Episode Score: 25,  Train Loss: 7.10950,  Curiosity Loss: 3.54097,  Time taken: 15 secs.\n",
      "(5.0/5)  Episode Score: 50,  Train Loss: 6.64158,  Curiosity Loss: 3.53350,  Time taken: 16 secs.\n",
      "Saved model at episode 5 as: 'rainbowcuriosity_batch32_buffer1k_Qbert_ep5.pt'.\n",
      "Saved logger data to 'saved_models/rainbowcuriosity_Qbert_logger_data.tar.gz'. Total size: 714 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "rainbow.train(num_episodes=5, print_every=1, save_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d03e319-49fd-419e-91b1-0c2e73747120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on Qbert with 5 episodes.\n",
      "Buffer size: 1k, batch size: 32, max timesteps: 1k, num network updates: 4, replay period: 100, intrinsic method: None.\n",
      "(1.0/5)  Episode Score: 125,  Train Loss: 3.73911,  Time taken: 8 secs.\n",
      "(2.0/5)  Episode Score: 25,  Train Loss: 3.77298,  Time taken: 5 secs.\n",
      "(3.0/5)  Episode Score: 150,  Train Loss: 3.16742,  Time taken: 6 secs.\n",
      "(4.0/5)  Episode Score: 200,  Train Loss: 2.54698,  Time taken: 7 secs.\n",
      "(5.0/5)  Episode Score: 125,  Train Loss: 2.80329,  Time taken: 6 secs.\n",
      "Saved model at episode 5 as: 'rainbow_batch32_buffer1k_Qbert_ep5.pt'.\n",
      "Saved logger data to 'saved_models/rainbow_Qbert_logger_data.tar.gz'. Total size: 667 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "rainbow2.train(num_episodes=5, print_every=1, save_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef83f451-d57f-4467-89cc-f904edb15c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on Qbert with 50 episodes.\n",
      "Buffer size: 1k, batch size: 32, max timesteps: 1k, num network updates: 4, replay period: 100, intrinsic method: empowerment.\n",
      "(1.0/50)  Episode Score: 375,  Train Loss: 3.87390,  Source Loss: 0.00107,  Forward Loss: 1.24322,  Time taken: 7 secs.\n",
      "(5.0/50)  Episode Score: 225,  Train Loss: 2.53961,  Source Loss: 0.00009,  Forward Loss: 0.69833,  Time taken: 42 secs.\n",
      "(10.0/50)  Episode Score: 100,  Train Loss: 1.38658,  Source Loss: 0.00005,  Forward Loss: 2.22284,  Time taken: 31 secs.\n",
      "(15.0/50)  Episode Score: 175,  Train Loss: 1.82600,  Source Loss: 0.00002,  Forward Loss: 0.71608,  Time taken: 35 secs.\n",
      "(20.0/50)  Episode Score: 750,  Train Loss: 0.54478,  Source Loss: 0.00001,  Forward Loss: 1.54741,  Time taken: 37 secs.\n",
      "(25.0/50)  Episode Score: 125,  Train Loss: 1.81434,  Source Loss: 0.00001,  Forward Loss: 2.01735,  Time taken: 27 secs.\n",
      "(30.0/50)  Episode Score: 175,  Train Loss: 1.86770,  Source Loss: 0.00001,  Forward Loss: 1.07069,  Time taken: 36 secs.\n",
      "(35.0/50)  Episode Score: 375,  Train Loss: 2.38931,  Source Loss: 0.00000,  Forward Loss: 0.92647,  Time taken: 2 mins, 41 secs.\n",
      "(40.0/50)  Episode Score: 250,  Train Loss: 2.14235,  Source Loss: 0.00000,  Forward Loss: 0.78339,  Time taken: 29 secs.\n",
      "(45.0/50)  Episode Score: 175,  Train Loss: 1.17091,  Source Loss: 0.00000,  Forward Loss: 0.63520,  Time taken: 35 secs.\n",
      "(50.0/50)  Episode Score: 300,  Train Loss: 1.29443,  Source Loss: 0.00000,  Forward Loss: 0.62218,  Time taken: 38 secs.\n",
      "Saved model at episode 50 as: 'rainbowempowerment_batch32_buffer1k_Qbert_ep50.pt'.\n",
      "Saved logger data to 'saved_models/rainbowempowerment_Qbert_logger_data.tar.gz'. Total size: 1873 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "rainbow3.train(num_episodes=50, print_every=5, save_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "689a2d12-602d-434c-a923-289196252790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Available attributes: '['avg_returns', 'actions', 'train_losses', 'ep_scores', 'intrinsic_losses']',\n",
       " Available attributes: '['avg_returns', 'actions', 'train_losses', 'ep_scores', 'intrinsic_losses']',\n",
       " Available attributes: '['avg_returns', 'actions', 'train_losses', 'ep_scores', 'intrinsic_losses']')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rainbow.logger, rainbow2.logger, rainbow3.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5013ad7-4a46-4ddb-89fd-6cac5a4f90f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Counter({4: 4273, 0: 227, 2: 917, 5: 2543, 1: 1171, 3: 117})],\n",
       " [Counter({4: 3040, 0: 193, 2: 2344, 3: 2213, 5: 1428, 1: 62})],\n",
       " [Counter({5: 28685, 0: 18801, 3: 21871, 4: 12482, 1: 16860, 2: 15765})])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rainbow.logger.actions, rainbow2.logger.actions, rainbow3.logger.actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76989e54-1c8d-47e2-b1b6-371b6f9cb4d5",
   "metadata": {},
   "source": [
    "### 2c. Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b051aab5-79eb-42d2-8054-4384c256843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PPO instance\n",
    "ppo = create_model('ppo', env=env3, device=device, im_type='curiosity')\n",
    "ppo2 = create_model('ppo', env=env3, device=device)\n",
    "ppo3 = create_model('ppo', env=env3, device=device, im_type='empowerment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df8ae064-8fab-40ae-a0df-e1f20de41e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'gym_name': 'ALE/MontezumaRevenge-v5', 'name': 'MontezumaRevenge', 'obs_space': Box(0, 255, (4, 84, 84), uint8), 'action_space': Discrete(18), 'input_shape': (4, 84, 84), 'n_actions': 18, 'img_size': 84, 'stack_size': 4, 'capture_video': False, 'record_every': 10000},\n",
       " {'gym_name': 'ALE/MontezumaRevenge-v5', 'name': 'MontezumaRevenge', 'obs_space': Box(0, 255, (4, 84, 84), uint8), 'action_space': Discrete(18), 'input_shape': (4, 84, 84), 'n_actions': 18, 'img_size': 84, 'stack_size': 4, 'capture_video': False, 'record_every': 10000},\n",
       " {'gym_name': 'ALE/MontezumaRevenge-v5', 'name': 'MontezumaRevenge', 'obs_space': Box(0, 255, (4, 84, 84), uint8), 'action_space': Discrete(18), 'input_shape': (4, 84, 84), 'n_actions': 18, 'img_size': 84, 'stack_size': 4, 'capture_video': False, 'record_every': 10000})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.env_details, ppo2.env_details, ppo3.env_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e0bec5f-9f1d-4402-97ba-985fa47d3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_NUM_EPISODES = ppo.params.rollout_size * ppo.params.num_envs * NUM_EPISODES\n",
    "demo_episodes = int((PPO_NUM_EPISODES / NUM_EPISODES) * 5) # 5 training iterations\n",
    "demo_episodes2 = int((PPO_NUM_EPISODES / NUM_EPISODES) * 50) # 5 training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30ac281a-128b-474d-962f-580b1a400bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on MontezumaRevenge with 200 episodes.\n",
      "Surrogate clipping size: 0.1, rollout size: 10, num environments: 4, num network updates: 4, batch size: 40, training iterations: 5, intrinsic method: curiosity.\n",
      "(1.0/5) Episode Score: 0.00,  Episodic Return: 0.05,  Approx KL: 0.006,  Total Loss: 7862.735,  Policy Loss: -0.001,  Value Loss: 5255.996,  Entropy Loss: 2.889,  Curiosity Loss: 5197.06201,  Time taken: 1 secs.\n",
      "(2.0/5) Episode Score: 0.00,  Episodic Return: 0.31,  Approx KL: 0.020,  Total Loss: 7912.451,  Policy Loss: -0.117,  Value Loss: 5304.566,  Entropy Loss: 2.851,  Curiosity Loss: 5283.66455,  Time taken: 1 secs.\n",
      "(3.0/5) Episode Score: 0.00,  Episodic Return: 2.32,  Approx KL: 2.133,  Total Loss: 7863.190,  Policy Loss: -0.245,  Value Loss: 5244.028,  Entropy Loss: 1.230,  Curiosity Loss: 5238.63428,  Time taken: 1 secs.\n",
      "(4.0/5) Episode Score: 0.00,  Episodic Return: 17.46,  Approx KL: -0.000,  Total Loss: 7663.471,  Policy Loss: 0.000,  Value Loss: 4947.016,  Entropy Loss: 0.000,  Curiosity Loss: 5208.86963,  Time taken: 1 secs.\n",
      "(5.0/5) Episode Score: 0.00,  Episodic Return: 62.67,  Approx KL: 0.000,  Total Loss: 7452.097,  Policy Loss: -0.000,  Value Loss: 4524.267,  Entropy Loss: 0.000,  Curiosity Loss: 5197.41943,  Time taken: 2 secs.\n",
      "Saved model at episode 5 as: 'ppocuriosity_rollout10_agents4_MonRev_ep5.pt'.\n",
      "Saved logger data to 'saved_models/ppocuriosity_MonRev_logger_data.tar.gz'. Total size: 930 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "ppo.train(num_episodes=demo_episodes, print_every=1, save_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5cd5641-1511-4e96-9934-f9f4087f6ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on MontezumaRevenge with 200 episodes.\n",
      "Surrogate clipping size: 0.1, rollout size: 10, num environments: 4, num network updates: 4, batch size: 40, training iterations: 5, intrinsic method: None.\n",
      "(1.0/5) Episode Score: 0.00,  Episodic Return: -0.05,  Approx KL: 0.001,  Total Loss: -0.040,  Policy Loss: -0.011,  Value Loss: 0.000,  Entropy Loss: 2.889,  Time taken: 0 secs.\n",
      "(2.0/5) Episode Score: 0.00,  Episodic Return: -0.05,  Approx KL: 0.011,  Total Loss: -0.046,  Policy Loss: -0.017,  Value Loss: 0.000,  Entropy Loss: 2.886,  Time taken: 0 secs.\n",
      "(3.0/5) Episode Score: 0.00,  Episodic Return: -0.03,  Approx KL: 0.209,  Total Loss: -0.172,  Policy Loss: -0.145,  Value Loss: 0.000,  Entropy Loss: 2.691,  Time taken: 0 secs.\n",
      "(4.0/5) Episode Score: 0.00,  Episodic Return: -0.03,  Approx KL: 2.401,  Total Loss: -0.218,  Policy Loss: -0.211,  Value Loss: 0.002,  Entropy Loss: 0.733,  Time taken: 0 secs.\n",
      "(5.0/5) Episode Score: 0.00,  Episodic Return: -0.13,  Approx KL: -0.000,  Total Loss: 0.005,  Policy Loss: -0.000,  Value Loss: 0.010,  Entropy Loss: 0.000,  Time taken: 0 secs.\n",
      "Saved model at episode 5 as: 'ppo_rollout10_agents4_MonRev_ep5.pt'.\n",
      "Saved logger data to 'saved_models/ppo_MonRev_logger_data.tar.gz'. Total size: 920 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "ppo2.train(num_episodes=demo_episodes, print_every=1, save_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1dcc3d76-787b-4630-8ea1-53fb320e09a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on MontezumaRevenge with 2K episodes.\n",
      "Surrogate clipping size: 0.1, rollout size: 10, num environments: 4, num network updates: 4, batch size: 40, training iterations: 50, intrinsic method: empowerment.\n",
      "(1.0/50) Episode Score: 0.00,  Episodic Return: 0.02,  Approx KL: 0.001,  Total Loss: -0.035,  Policy Loss: -0.006,  Value Loss: 0.000,  Entropy Loss: 2.882,  Source Loss: 0.20103,  Forward Loss: 1.30656,  Time taken: 0 secs.\n",
      "(5.0/50) Episode Score: 0.00,  Episodic Return: 0.03,  Approx KL: -0.001,  Total Loss: -0.033,  Policy Loss: -0.005,  Value Loss: 0.000,  Entropy Loss: 2.879,  Source Loss: 0.02357,  Forward Loss: 1.22585,  Time taken: 2 secs.\n",
      "(10.0/50) Episode Score: 0.00,  Episodic Return: 0.02,  Approx KL: -0.000,  Total Loss: -0.031,  Policy Loss: -0.002,  Value Loss: 0.000,  Entropy Loss: 2.875,  Source Loss: 0.01383,  Forward Loss: 1.38899,  Time taken: 2 secs.\n",
      "(15.0/50) Episode Score: 0.00,  Episodic Return: 0.03,  Approx KL: -0.004,  Total Loss: -0.033,  Policy Loss: -0.004,  Value Loss: 0.000,  Entropy Loss: 2.880,  Source Loss: 0.02004,  Forward Loss: 1.30302,  Time taken: 2 secs.\n",
      "(20.0/50) Episode Score: 0.00,  Episodic Return: 0.03,  Approx KL: 0.003,  Total Loss: -0.038,  Policy Loss: -0.010,  Value Loss: 0.000,  Entropy Loss: 2.859,  Source Loss: 0.01290,  Forward Loss: 1.00017,  Time taken: 2 secs.\n",
      "(25.0/50) Episode Score: 0.00,  Episodic Return: 0.03,  Approx KL: 0.014,  Total Loss: -0.040,  Policy Loss: -0.013,  Value Loss: 0.000,  Entropy Loss: 2.786,  Source Loss: 0.01551,  Forward Loss: 0.54402,  Time taken: 2 secs.\n",
      "(30.0/50) Episode Score: 0.00,  Episodic Return: 0.03,  Approx KL: 0.099,  Total Loss: -0.106,  Policy Loss: -0.085,  Value Loss: 0.000,  Entropy Loss: 2.166,  Source Loss: 0.01329,  Forward Loss: 0.47022,  Time taken: 2 secs.\n",
      "(35.0/50) Episode Score: 0.00,  Episodic Return: 0.02,  Approx KL: 0.002,  Total Loss: -0.011,  Policy Loss: -0.002,  Value Loss: 0.000,  Entropy Loss: 0.918,  Source Loss: 0.01455,  Forward Loss: 0.46085,  Time taken: 2 secs.\n",
      "(40.0/50) Episode Score: 0.00,  Episodic Return: 0.01,  Approx KL: 0.015,  Total Loss: -0.034,  Policy Loss: -0.013,  Value Loss: 0.000,  Entropy Loss: 2.052,  Source Loss: 0.01449,  Forward Loss: 0.17895,  Time taken: 2 secs.\n",
      "(45.0/50) Episode Score: 0.00,  Episodic Return: 0.01,  Approx KL: 0.003,  Total Loss: -0.016,  Policy Loss: 0.002,  Value Loss: 0.000,  Entropy Loss: 1.827,  Source Loss: 0.01787,  Forward Loss: 0.06449,  Time taken: 2 secs.\n",
      "(50.0/50) Episode Score: 0.00,  Episodic Return: -0.00,  Approx KL: 0.013,  Total Loss: -0.026,  Policy Loss: 0.001,  Value Loss: 0.000,  Entropy Loss: 2.643,  Source Loss: 0.01471,  Forward Loss: 0.47001,  Time taken: 2 secs.\n",
      "Saved model at episode 50 as: 'ppoempowerment_rollout10_agents4_MonRev_ep50.pt'.\n",
      "Saved logger data to 'saved_models/ppoempowerment_MonRev_logger_data.tar.gz'. Total size: 3053 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "ppo3.train(num_episodes=demo_episodes2, print_every=5, save_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85944b0b-ee91-448f-ac25-24973439e5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Available attributes: '['actions', 'avg_returns', 'avg_rewards', 'policy_losses', 'value_losses', 'entropy_losses', 'total_losses', 'approx_kl', 'intrinsic_losses']',\n",
       " Available attributes: '['actions', 'avg_returns', 'avg_rewards', 'policy_losses', 'value_losses', 'entropy_losses', 'total_losses', 'approx_kl', 'intrinsic_losses']',\n",
       " Available attributes: '['actions', 'avg_returns', 'avg_rewards', 'policy_losses', 'value_losses', 'entropy_losses', 'total_losses', 'approx_kl', 'intrinsic_losses']')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.logger, ppo2.logger, ppo3.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8bae015-9472-4358-bb30-b05cc11d916f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Counter({4: 44,\n",
       "           5: 24,\n",
       "           9: 44,\n",
       "           3: 32,\n",
       "           12: 24,\n",
       "           8: 16,\n",
       "           17: 40,\n",
       "           14: 20,\n",
       "           16: 28,\n",
       "           0: 16,\n",
       "           7: 20,\n",
       "           10: 36,\n",
       "           13: 20,\n",
       "           11: 16,\n",
       "           2: 20,\n",
       "           1: 28,\n",
       "           15: 8,\n",
       "           6: 364})],\n",
       " [Counter({16: 12,\n",
       "           10: 32,\n",
       "           4: 64,\n",
       "           9: 28,\n",
       "           14: 44,\n",
       "           1: 28,\n",
       "           11: 24,\n",
       "           0: 36,\n",
       "           12: 28,\n",
       "           3: 36,\n",
       "           13: 24,\n",
       "           8: 28,\n",
       "           5: 24,\n",
       "           7: 44,\n",
       "           15: 32,\n",
       "           17: 24,\n",
       "           6: 64,\n",
       "           2: 228})],\n",
       " [Counter({11: 308,\n",
       "           9: 368,\n",
       "           6: 188,\n",
       "           15: 168,\n",
       "           1: 332,\n",
       "           17: 316,\n",
       "           7: 1564,\n",
       "           14: 236,\n",
       "           4: 492,\n",
       "           10: 1484,\n",
       "           13: 360,\n",
       "           3: 280,\n",
       "           2: 404,\n",
       "           5: 232,\n",
       "           8: 404,\n",
       "           12: 324,\n",
       "           16: 200,\n",
       "           0: 340})])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.logger.actions, ppo2.logger.actions, ppo3.logger.actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7141e-6cb6-478e-b444-00d616c2cb2b",
   "metadata": {},
   "source": [
    "## 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853b5f0-4285-4320-94b2-8961a7632bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.render import video_render\n",
    "from utils.model_utils import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd90ef-06c6-4723-8a82-b95b0124634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('rainbowcuriosity_batch32_buffer1k_Qbert_ep5', 'cuda:0', 'rainbow')\n",
    "# model = load_model('dqncuriosity_batch32_SpaInv_ep5', 'cuda:0', 'dqn')\n",
    "# model = load_model('ppocuriosity_rollout100_agents8_MonRev_ep5', 'cuda:0', 'ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187de70-2602-488d-bb2f-7cca1c257787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_render(model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f4d11-d808-4e98-8ea7-79f97773a716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlatari",
   "language": "python",
   "name": "rlatari"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
