{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed87b53b-9602-4c27-ba3f-d9d40ff4279a",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Atari Games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a621639-4192-4fce-895e-14b335bfe7ff",
   "metadata": {},
   "source": [
    "## 1. Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9380453d-9c2b-41fb-b9f3-b7d739143812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.create import create_model, get_utility_params\n",
    "from utils.helper import set_device\n",
    "from utils.model_utils import load_model\n",
    "from utils.render import video_render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4292c565-0b2d-45ed-98bf-4b43d7cb5824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_EPISODES=50000, SAVE_EVERY=10000\n"
     ]
    }
   ],
   "source": [
    "# Get utility parameters from yaml file\n",
    "util_params = get_utility_params()\n",
    "\n",
    "# Set them as hyperparameters\n",
    "NUM_EPISODES = util_params['num_episodes']\n",
    "SAVE_EVERY = util_params['save_every']\n",
    "print(f'NUM_EPISODES={NUM_EPISODES}, SAVE_EVERY={SAVE_EVERY}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1030fb-367b-4956-a090-2ecfc8a80eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available. Device set to GPU -> 'cuda:0'\n"
     ]
    }
   ],
   "source": [
    "# Set CUDA device\n",
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb40765c-056c-4a44-9638-18528853cb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env2=ALE/Qbert-v5, env3=ALE/MontezumaRevenge-v5\n"
     ]
    }
   ],
   "source": [
    "env2 = util_params['env_2']\n",
    "env3 = util_params['env_3']\n",
    "print(f'env2={env2}, env3={env3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da63c80-1fd1-4579-b18a-d5fa6c20f1a0",
   "metadata": {},
   "source": [
    "## 2. Model Creation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371fd8e-4667-4bd0-93f9-99770ce70873",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2a. Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4d531e1-2dd7-4ffa-9e9a-3ae773844c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Frazzle\\anaconda3\\envs\\rla2\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create DQN instance\n",
    "dqn = create_model('dqn', device=device, im_type='curiosity')\n",
    "dqn2 = create_model('dqn', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c75b7a4-5c68-4316-820a-b459ee30dafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gym_name': 'ALE/SpaceInvaders-v5', 'name': 'SpaceInvaders', 'obs_space': Box(0, 255, (4, 84, 84), uint8), 'action_space': Discrete(6), 'input_shape': (4, 84, 84), 'n_actions': 6, 'img_size': 84, 'stack_size': 4, 'capture_video': False, 'record_every': 10000}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.env_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56824d63-5850-400d-ac3a-4684b378bbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on SpaceInvaders with 5 episodes.\n",
      "Buffer size: 1k, batch size: 32, max timesteps: 1k, num network updates: 4, intrinsic method: curiosity\n",
      "(1.0/5) Episode Score: 120,  Train Loss: 2426.58936,  Curiosity Loss: 3.57615,  Time taken: 5.83 secs.\n",
      "(2.0/5) Episode Score: 65,  Train Loss: 308.26587,  Curiosity Loss: 3.57719,  Time taken: 3.27 secs.\n",
      "(3.0/5) Episode Score: 80,  Train Loss: 251.28535,  Curiosity Loss: 3.57784,  Time taken: 2.72 secs.\n",
      "(4.0/5) Episode Score: 155,  Train Loss: 173.95744,  Curiosity Loss: 3.57762,  Time taken: 4.03 secs.\n",
      "(5.0/5) Episode Score: 155,  Train Loss: 16.91314,  Curiosity Loss: 3.57728,  Time taken: 3.91 secs.\n",
      "Saved model at episode 5 as: 'dqncuriosity_batch32_SpaInv_ep5.pt'.\n",
      "Saved logger data to 'saved_models/dqncuriosity_SpaInv_logger_data.tar.gz'. Total size: 843 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "dqn.train(num_episodes=5, print_every=1, save_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d23ebf8-43b6-46e8-a14f-68c01e391598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on SpaceInvaders with 5 episodes.\n",
      "Buffer size: 1k, batch size: 32, max timesteps: 1k, num network updates: 4, intrinsic method: None\n",
      "(1.0/5) Episode Score: 210,  Train Loss: 17073.07812,  Time taken: 2.23 secs.\n",
      "(2.0/5) Episode Score: 35,  Train Loss: 1859.45862,  Time taken: 1.39 secs.\n",
      "(3.0/5) Episode Score: 50,  Train Loss: 1251.52161,  Time taken: 1.48 secs.\n",
      "(4.0/5) Episode Score: 90,  Train Loss: 257.93082,  Time taken: 2.42 secs.\n",
      "(5.0/5) Episode Score: 205,  Train Loss: 21.34808,  Time taken: 3.44 secs.\n",
      "Saved model at episode 5 as: 'dqn_batch32_SpaInv_ep5.pt'.\n",
      "Saved logger data to 'saved_models/dqn_SpaInv_logger_data.tar.gz'. Total size: 794 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "dqn2.train(num_episodes=5, print_every=1, save_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea56055b-a0d3-4b58-9c5e-c74c3d1083d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Available attributes: '['actions', 'train_losses', 'ep_scores', 'intrinsic_losses']'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e59db80a-3efa-4e8f-9053-24691b685117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({4: 385, 1: 416, 0: 423, 2: 390, 5: 406, 3: 455})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.logger.actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60affa60-f650-4b74-89de-d59f8a6b7b04",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2b. Rainbow Deep Q-Network (RDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "725535d6-9d22-4398-85fc-0ffb43650a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Rainbow DQN instance\n",
    "rainbow = create_model('rainbow', env=env2, device=device, im_type='curiosity')\n",
    "rainbow2 = create_model('rainbow', env=env2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aad46cd-adc2-4828-b283-2cd7fc2fbc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gym_name': 'ALE/Qbert-v5', 'name': 'Qbert', 'obs_space': Box(0, 255, (4, 84, 84), uint8), 'action_space': Discrete(6), 'input_shape': (4, 84, 84), 'n_actions': 6, 'img_size': 84, 'stack_size': 4, 'capture_video': False, 'record_every': 10000}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rainbow.env_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f681e4b3-54d7-4fc8-91a3-69a7162f1f54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on Qbert with 5 episodes.\n",
      "Buffer size: 1k, batch size: 32, max timesteps: 1k, num network updates: 4, replay period: 100, intrinsic method: curiosity.\n",
      "(1.0/5)  Episode Score: 0,  Train Loss: 7.43996,  Curiosity Loss: 3.53157,  Time taken: 4.72 secs.\n",
      "(2.0/5)  Episode Score: 250,  Train Loss: 7.36464,  Curiosity Loss: 3.54006,  Time taken: 6.64 secs.\n",
      "(3.0/5)  Episode Score: 125,  Train Loss: 7.14056,  Curiosity Loss: 3.53956,  Time taken: 4.30 secs.\n",
      "(4.0/5)  Episode Score: 175,  Train Loss: 6.85037,  Curiosity Loss: 3.54415,  Time taken: 4.61 secs.\n",
      "(5.0/5)  Episode Score: 175,  Train Loss: 5.20295,  Curiosity Loss: 3.54321,  Time taken: 5.11 secs.\n",
      "Saved model at episode 5 as: 'rainbowcuriosity_batch32_buffer1k_Qbert_ep5.pt'.\n",
      "Saved logger data to 'saved_models/rainbowcuriosity_Qbert_logger_data.tar.gz'. Total size: 708 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "rainbow.train(num_episodes=5, print_every=1, save_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d03e319-49fd-419e-91b1-0c2e73747120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on Qbert with 5 episodes.\n",
      "Buffer size: 1k, batch size: 32, max timesteps: 1k, num network updates: 4, replay period: 100, intrinsic method: None.\n",
      "(1.0/5)  Episode Score: 175,  Train Loss: 3.71781,  Time taken: 4.32 secs.\n",
      "(2.0/5)  Episode Score: 125,  Train Loss: 3.80244,  Time taken: 3.58 secs.\n",
      "(3.0/5)  Episode Score: 325,  Train Loss: 3.20816,  Time taken: 4.20 secs.\n",
      "(4.0/5)  Episode Score: 125,  Train Loss: 3.06909,  Time taken: 3.49 secs.\n",
      "(5.0/5)  Episode Score: 25,  Train Loss: 2.56938,  Time taken: 4.69 secs.\n",
      "Saved model at episode 5 as: 'rainbow_batch32_buffer1k_Qbert_ep5.pt'.\n",
      "Saved logger data to 'saved_models/rainbow_Qbert_logger_data.tar.gz'. Total size: 669 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "rainbow2.train(num_episodes=5, print_every=1, save_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "689a2d12-602d-434c-a923-289196252790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Available attributes: '['avg_returns', 'actions', 'train_losses', 'ep_scores', 'intrinsic_losses']'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rainbow.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5013ad7-4a46-4ddb-89fd-6cac5a4f90f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Counter({4: 3268, 0: 165, 2: 1150, 5: 1339, 3: 3457, 1: 29})],\n",
       " [Counter({5: 4121, 2: 1149, 3: 2054, 4: 1367, 0: 599, 1: 54})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rainbow.logger.actions, rainbow2.logger.actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76989e54-1c8d-47e2-b1b6-371b6f9cb4d5",
   "metadata": {},
   "source": [
    "### 2c. Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b051aab5-79eb-42d2-8054-4384c256843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PPO instance\n",
    "ppo = create_model('ppo', env=env3, device=device, im_type='curiosity')\n",
    "ppo2 = create_model('ppo', env=env3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df8ae064-8fab-40ae-a0df-e1f20de41e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gym_name': 'ALE/MontezumaRevenge-v5', 'name': 'MontezumaRevenge', 'obs_space': Box(0, 255, (4, 84, 84), uint8), 'action_space': Discrete(18), 'input_shape': (4, 84, 84), 'n_actions': 18, 'img_size': 84, 'stack_size': 4, 'capture_video': False, 'record_every': 10000}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.env_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e0bec5f-9f1d-4402-97ba-985fa47d3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_NUM_EPISODES = ppo.params.rollout_size * ppo.params.num_envs * NUM_EPISODES\n",
    "demo_episodes = int((PPO_NUM_EPISODES / NUM_EPISODES) * 5) # 5 training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30ac281a-128b-474d-962f-580b1a400bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on MontezumaRevenge with 4K episodes.\n",
      "Surrogate clipping size: 0.1, rollout size: 100, num environments: 8, num network updates: 4, batch size: 800, training iterations: 5, intrinsic method: curiosity.\n",
      "(1.0/5) Episodic Return: -0.03268,  Approx KL: -0.00050,  Total Loss: 8451.31329,  Policy Loss: -0.00067,  Value Loss: 5856.00031,  Entropy Loss: 2.88883,  Curiosity Loss: 5523.34271,  Time taken: 2.23 secs.\n",
      "(2.0/5) Episodic Return: 0.16280,  Approx KL: 0.04365,  Total Loss: 8423.76660,  Policy Loss: -0.05518,  Value Loss: 5821.33765,  Entropy Loss: 2.82460,  Curiosity Loss: 5513.18121,  Time taken: 1.76 secs.\n",
      "(3.0/5) Episodic Return: 2.12083,  Approx KL: 3.06891,  Total Loss: 8350.86261,  Policy Loss: -0.38449,  Value Loss: 5685.05963,  Entropy Loss: 0.79983,  Curiosity Loss: 5508.72528,  Time taken: 1.74 secs.\n",
      "(4.0/5) Episodic Return: 16.31793,  Approx KL: -0.00002,  Total Loss: 8203.01459,  Policy Loss: 0.00000,  Value Loss: 5101.84595,  Entropy Loss: 0.00002,  Curiosity Loss: 5652.09161,  Time taken: 1.76 secs.\n",
      "(5.0/5) Episodic Return: 45.06052,  Approx KL: 0.00000,  Total Loss: 7447.14059,  Policy Loss: -0.00000,  Value Loss: 3590.09808,  Entropy Loss: 0.00000,  Curiosity Loss: 5652.09158,  Time taken: 1.76 secs.\n",
      "Saved model at episode 5 as: 'ppocuriosity_rollout100_agents8_MonRev_ep5.pt'.\n",
      "Saved logger data to 'saved_models/ppocuriosity_MonRev_logger_data.tar.gz'. Total size: 959 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "ppo.train(num_episodes=demo_episodes, print_every=1, save_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5cd5641-1511-4e96-9934-f9f4087f6ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent on MontezumaRevenge with 4K episodes.\n",
      "Surrogate clipping size: 0.1, rollout size: 100, num environments: 8, num network updates: 4, batch size: 800, training iterations: 5, intrinsic method: None.\n",
      "(1.0/5) Episodic Return: -0.03268,  Approx KL: 0.00023,  Total Loss: -0.02987,  Policy Loss: -0.00104,  Value Loss: 0.00012,  Entropy Loss: 2.88933,  Time taken: 1.12 secs.\n",
      "(2.0/5) Episodic Return: -0.02270,  Approx KL: -0.00004,  Total Loss: -0.02925,  Policy Loss: -0.00038,  Value Loss: 0.00005,  Entropy Loss: 2.88914,  Time taken: 1.17 secs.\n",
      "(3.0/5) Episodic Return: -0.01221,  Approx KL: -0.00001,  Total Loss: -0.02946,  Policy Loss: -0.00058,  Value Loss: 0.00001,  Entropy Loss: 2.88902,  Time taken: 1.15 secs.\n",
      "(4.0/5) Episodic Return: -0.00705,  Approx KL: -0.00025,  Total Loss: -0.02917,  Policy Loss: -0.00029,  Value Loss: 0.00000,  Entropy Loss: 2.88858,  Time taken: 1.09 secs.\n",
      "(5.0/5) Episodic Return: -0.00253,  Approx KL: -0.00014,  Total Loss: -0.02937,  Policy Loss: -0.00049,  Value Loss: 0.00000,  Entropy Loss: 2.88862,  Time taken: 1.08 secs.\n",
      "Saved model at episode 5 as: 'ppo_rollout100_agents8_MonRev_ep5.pt'.\n",
      "Saved logger data to 'saved_models/ppo_MonRev_logger_data.tar.gz'. Total size: 913 bytes\n",
      "Training complete. Access metrics from 'logger' attribute. "
     ]
    }
   ],
   "source": [
    "ppo2.train(num_episodes=demo_episodes, print_every=1, save_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85944b0b-ee91-448f-ac25-24973439e5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Available attributes: '['actions', 'avg_rewards', 'avg_returns', 'policy_losses', 'value_losses', 'entropy_losses', 'total_losses', 'approx_kl', 'intrinsic_losses']'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8bae015-9472-4358-bb30-b05cc11d916f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8451.313293457031,\n",
       "  8423.7666015625,\n",
       "  8350.862609863281,\n",
       "  8203.014587402344,\n",
       "  7447.140594482422],\n",
       " [-0.029874820553231984,\n",
       "  -0.029246532707475126,\n",
       "  -0.029461760306730866,\n",
       "  -0.029169123619794846,\n",
       "  -0.029371113108936697])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.logger.total_losses, ppo2.logger.total_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7141e-6cb6-478e-b444-00d616c2cb2b",
   "metadata": {},
   "source": [
    "## 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6cd90ef-06c6-4723-8a82-b95b0124634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('', device, 'dqn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b187de70-2602-488d-bb2f-7cca1c257787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_render(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rla2",
   "language": "python",
   "name": "rla2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
