# Environment
ENV_1="ALE/SpaceInvaders-v5"
ENV_2="ALE/Qbert-v5"
ENV_3="ALE/MontezumaRevenge-v5"
IMG_SIZE=128
STACK_SIZE=4
CAPTURE_VIDEO=False # Video recording

# Generic
GAMMA=0.99 # Discount factor
LEARNING_RATE=1e-3
EPSILON=1e-3
NUM_EPISODES=10000  # (total timesteps)
SAVE_EVERY=1000 # Number of episodes to save the model
SEED=368

# Used by both agents
UPDATE_STEPS=4 # How often to update the network

# DQN
TAU=1e-3 # Soft updater for target network
BUFFER_SIZE=1e5 # Replay buffer size
BATCH_SIZE=32 # Buffer training batch size
EPS_START= 1.0 # Initial epsilon
EPS_END=0.01 # Greedy epsilon threshold
EPS_DECAY=0.995 # Epsilon decay rate
MAX_TIMESTEPS=1000 # Max before episode end

# PPO
LOSS_CLIP=0.1 # Value for surrogate clipping
ROLLOUT_SIZE=10 # Number of samples to train on
NUM_AGENTS=8 # Number of agents used during training
NUM_MINI_BATCHES=4 # Number of mini-batches during training
ENTROPY_COEF=0.01 # Coefficient for regularisation (lambda)
VALUE_LOSS_COEF=0.5 # Coefficient for decreasing value loss
CLIP_GRAD=0.5 # Maximum value for gradient clipping
