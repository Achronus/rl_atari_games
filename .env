# Environment
ENV_1="ALE/SpaceInvaders-v5"
ENV_2="ALE/Qbert-v5"
ENV_3="ALE/MontezumaRevenge-v5"
IMG_SIZE=128
STACK_SIZE=4
CAPTURE_VIDEO=False # Video recording
RECORD_EVERY=1000 # Number of episodes between each video recording

# Generic
NUM_EPISODES=10000
GAMMA=0.99 # Discount factor
LEARNING_RATE=1e-3
EPSILON=1e-3
SEED=368
UPDATE_STEPS=4 # How often to update the network
CLIP_GRAD=0.5 # Maximum value for gradient clipping

# DQN
TAU=1e-3 # Soft updater for target network
EPS_START=1.0 # Initial epsilon
EPS_END=0.01 # Greedy epsilon threshold
EPS_DECAY=0.995 # Epsilon decay rate
MAX_TIMESTEPS=1000 # Maximum steps before each episode ends

# RDQN
N_STEPS=3 # Number of steps for multi-step learning
REPLAY_PERIOD=100 # Number of transitions before learning begins
LEARN_FREQUENCY=4 # Number of timesteps to perform agent learning
REWARD_CLIP=0.1 # Number for maximum reward bounds

# Prioritized Replay Buffer
BUFFER_SIZE=1000 # Replay buffer size
BATCH_SIZE=32 # Buffer mini-batch size
PRIORITY_EXPONENT=0.5 # Prioritized buffer exponent (alpha)
PRIORITY_WEIGHT=0.4 # Initial prioritized buffer importance sampling weight (beta)

# Categorical DQN
N_ATOMS=51 # Number of atoms (distributions)
V_MIN=-10 # Minimum size of the atoms
V_MAX=10 # Maximum size of the atoms

# PPO
LOSS_CLIP=0.1 # Value for surrogate clipping
ROLLOUT_SIZE=100 # Number of samples to train on
NUM_AGENTS=8 # Number of agents used during training
ENTROPY_COEF=0.01 # Coefficient for regularisation (lambda)
VALUE_LOSS_COEF=0.5 # Coefficient for decreasing value loss
NUM_MINI_BATCHES=4 # Number of mini-batches during training
